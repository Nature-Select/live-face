// Frame Selector - Unified Animation Frame Selection
//
// Áªü‰∏ÄÁöÑÂ∏ßÈÄâÊã©Âô®ÔºåÂ∞ÅË£ÖÊâÄÊúâÂä®ÁîªÁä∂ÊÄÅÊõ¥Êñ∞ÈÄªËæë„ÄÇ
//
// Ê†∏ÂøÉÁêÜÂøµÔºö
// 1. Á∫ØÂáΩÊï∞Â§ÑÁêÜÊØè‰∏ÄÂ∏ßÁöÑÊï∞ÊçÆ
// 2. ‰∏ç‰æùËµñÂ§ñÈÉ®Áä∂ÊÄÅÊú∫ÔºåËÄåÊòØÊé®ÂØºÂá∫ÂΩìÂâçÂ∫îËØ•Â§Ñ‰∫éÁöÑÁä∂ÊÄÅ
// 3. Ë∞ÉÁî®ÊñπË¥üË¥£ÂçèË∞ÉÁä∂ÊÄÅÊú∫‰∫ã‰ª∂Ëß¶Âèë

import 'package:flutter/foundation.dart';

import '../config/eyes_config.dart';
import '../config/mouth_config.dart';
import 'audio_analysis.dart';
import 'emotion_image_sets.dart';
import 'eyes_controller.dart';
import 'mouth_controller.dart';
import 'small_emotion_controller.dart';
import 'speech_history.dart';
import 'types.dart';
import 'vad.dart';

// ============================================================================
// Types
// ============================================================================

/// ËæìÂÖ•Êï∞ÊçÆ - ÊØè‰∏ÄÂ∏ßÁöÑÂéüÂßãÊï∞ÊçÆ
class FrameInput {
  /// Èü≥È¢ëÁâπÂæÅÔºàÊù•Ëá™ AudioAnalyzerÔºâ
  final VoiceMetrics audioMetrics;

  /// ÂæÖÂ§ÑÁêÜÁöÑÂ≠óÂπïÔºàÊñ∞Â≠óÂπïÂà∞ËææÊó∂ÊâçÊúâÂÄºÔºâ
  final PendingSubtitle? pendingSubtitle;

  /// üÜï ÂΩìÂâçËΩÆÊ¨°ÁöÑÂÖÉÊï∞ÊçÆÔºàÁî®‰∫é emoji/PAG Âà§Êñ≠Ôºâ
  final String? currentTurnEmoji;
  final int? currentTurnStatus;

  /// ÂΩìÂâçÂ∏ßÁºñÂè∑
  final int frameNumber;

  const FrameInput({
    required this.audioMetrics,
    this.pendingSubtitle,
    this.currentTurnEmoji,
    this.currentTurnStatus,
    required this.frameNumber,
  });
}

/// ÂæÖÂ§ÑÁêÜÁöÑÂ≠óÂπï
class PendingSubtitle {
  final String emotion;
  final String id;
  final String content;

  const PendingSubtitle({
    required this.emotion,
    required this.id,
    required this.content,
  });
}

/// ÊòæÁ§∫ÁöÑÂ≠óÂπï‰ø°ÊÅØ
class DisplayedSubtitle {
  final String id;
  final String content;
  final String emotion;
  final String? emoji; // üÜï
  final int? turnStatus; // üÜï

  const DisplayedSubtitle({
    required this.id,
    required this.content,
    required this.emotion,
    this.emoji,
    this.turnStatus,
  });
}

/// PAG Âä®ÁîªËæìÂá∫
class PAGOutput {
  final String src;
  final bool isPlaying;

  const PAGOutput({required this.src, required this.isPlaying});
}

/// Ë∞ÉËØï‰ø°ÊÅØ
class FrameDebugInfo {
  final MouthIntensity mouthState;
  final EyesState eyesState;
  final VoiceActivityState voiceActivity;
  final double energy;

  const FrameDebugInfo({
    required this.mouthState,
    required this.eyesState,
    required this.voiceActivity,
    required this.energy,
  });
}

/// ËæìÂá∫Êï∞ÊçÆ - Ê∏≤ÊüìÊâÄÈúÄÁöÑÊâÄÊúâ‰ø°ÊÅØ
class FrameOutput {
  /// ËßíËâ≤ÂõæÁâá URL
  final String imageUrl;

  /// PAG Âä®Áîª‰ø°ÊÅØÔºàÂ¶ÇÊûúÈúÄË¶ÅÊòæÁ§∫Ôºâ
  final PAGOutput? pag;

  /// üÜï Emoji ‰ø°ÊÅØÔºàÁî®‰∫é emoji overlay ÊòæÁ§∫Ôºâ
  final String? emoji;

  /// Ê£ÄÊµãÂà∞ÁöÑ‰ºöËØùÁä∂ÊÄÅÔºàÊé®ÂØºÁªìÊûúÔºâ
  final ActorState detectedState;

  /// ÊÉÖÁª™Ê†áÁ≠æÔºàÁî®‰∫éÂ≠óÂπïÊòæÁ§∫Ôºâ
  final String currentEmotion;

  /// ÊòØÂê¶ÊòæÁ§∫Êñ∞Â≠óÂπïÔºàpendingSubtitle Ë¢´Ê∂àË¥πÔºâ
  final bool shouldDisplaySubtitle;

  /// ÊòæÁ§∫ÁöÑÂ≠óÂπï‰ø°ÊÅØ
  final DisplayedSubtitle? displayedSubtitle;

  /// Ë∞ÉËØï‰ø°ÊÅØ
  final FrameDebugInfo? debug;

  const FrameOutput({
    required this.imageUrl,
    this.pag,
    this.emoji,
    required this.detectedState,
    required this.currentEmotion,
    required this.shouldDisplaySubtitle,
    this.displayedSubtitle,
    this.debug,
  });
}

// ============================================================================
// Frame Selector Class
// ============================================================================

/// Unified frame selector that coordinates all animation subsystems
class FrameSelector {
  final VadConfig vadConfig;
  final PauseDetectionConfig pauseDetectionConfig;

  // Internal Controllers
  late final MouthController _mouthController;
  late final EyesStateManager _eyesStateManager;
  late final VoiceActivityManager _voiceActivityManager;
  late final SpeechHistoryManager _speechHistoryManager;

  // Internal State
  String _currentEmotion = '[peace]';
  ActorState _currentDetectedState = ActorState.idle;

  // PAG Animation State
  String? _pagSrc;
  String? _lastSubtitleId;

  // üÜï Emoji State
  String? _currentEmoji;
  int _lastTurnStatus = 0;

  FrameSelector({
    required this.vadConfig,
    required this.pauseDetectionConfig,
    EyesLifecycleConfig? eyesLifecycleConfig,
    EyesTimingConfig? eyesTimingConfig,
    MouthConfig? mouthConfig,
  }) {
    // Initialize controllers with lifecycle state callback
    _mouthController = MouthController(
      getLifecycleState: () => _currentDetectedState,
      vadConfig: vadConfig,
      mouthConfig: mouthConfig,
    );

    _voiceActivityManager = VoiceActivityManager(
      VoiceActivityManagerConfig(
        debounceFrames: vadConfig.debounceFrames,
        smoothingFactor: vadConfig.smoothingFactor,
      ),
      VoiceActivityState.quiet,
    );

    _eyesStateManager = EyesStateManager(
      getLifecycleState: () => _currentDetectedState,
      lifecycleConfig: eyesLifecycleConfig,
      timingConfig: eyesTimingConfig,
    );

    _speechHistoryManager = SpeechHistoryManager(
      vadConfig: vadConfig,
      pauseDetectionConfig: pauseDetectionConfig,
    );
  }

  /// Select animation frame based on emotion, mouth state, and eyes state
  String _selectFrame(
    String emotion,
    MouthIntensity mouthState,
    EyesState eyesState,
  ) {
    // Get emotion image set
    final imageSet = getEmotionImageSet(emotion);
    if (imageSet == null) {
      throw Exception(
        "[ANIMATION] Emotion '$emotion' not found - backend data integrity error",
      );
    }

    // Select image based on eyes and mouth state
    final eyesClosed = eyesState == EyesState.closed;
    final mouthClosed = mouthState == MouthIntensity.closed;

    if (eyesClosed && !mouthClosed) {
      return imageSet.eyesClosedMouthOpen;
    } else if (eyesClosed && mouthClosed) {
      return imageSet.eyesClosedMouthClosed;
    } else if (!eyesClosed && !mouthClosed) {
      return imageSet.eyesOpenMouthOpen;
    } else {
      return imageSet.eyesOpenMouthClosed;
    }
  }

  /// Process a single frame and return rendering output
  FrameOutput processFrame(FrameInput input) {
    final audioMetrics = input.audioMetrics;
    final pendingSubtitle = input.pendingSubtitle;
    final frameNumber = input.frameNumber;

    final energy = audioMetrics.energy;
    final zcr = audioMetrics.zcr;

    // 1. Classify voice activity
    final newVoiceActivity = classifyVoiceActivity(energy, zcr, vadConfig);
    final smoothedActivity = _voiceActivityManager.smooth(newVoiceActivity);

    // 2. Process emoji/PAG only when turnStatus changes from 0 to 1
    final currentTurnStatus = input.currentTurnStatus ?? 0;
    if (currentTurnStatus == 1 && _lastTurnStatus != 1) {
      // Update emoji state
      if (input.currentTurnEmoji != null) {
        _currentEmoji = input.currentTurnEmoji;
        debugPrint('üòä [EMOJI] Set emoji in frameSelector: ${input.currentTurnEmoji}');
        debugPrint('üö´ [PAG] Skipped due to emoji priority');
      } else {
        // No emoji ‚Üí trigger PAG animation
        // ‰ΩøÁî® _currentEmotionÔºàÂ≠óÂπïÊòæÁ§∫Êó∂Â∑≤‰øùÂ≠òÔºâËÄå‰∏ç‰æùËµñ pendingSubtitle
        final emotionForPAG = _currentEmotion.isNotEmpty ? _currentEmotion : '[peace]';
        final idForPAG = DateTime.now().millisecondsSinceEpoch.toString();
        _triggerPAGAnimation(emotionForPAG, idForPAG);
        debugPrint('üé® [PAG] Lottery triggered (no emoji), emotion: $emotionForPAG');
      }
    }
    _lastTurnStatus = currentTurnStatus;

    // 3. Check if we should display pending subtitle (voice detected)
    bool shouldDisplaySubtitle = false;
    DisplayedSubtitle? displayedSubtitle;

    if (pendingSubtitle != null &&
        newVoiceActivity != VoiceActivityState.quiet) {
      // New subtitle arrives + voice detected ‚Üí display it
      shouldDisplaySubtitle = true;
      displayedSubtitle = DisplayedSubtitle(
        id: pendingSubtitle.id,
        content: pendingSubtitle.content,
        emotion: pendingSubtitle.emotion,
        emoji: input.currentTurnEmoji,
        turnStatus: input.currentTurnStatus,
      );

      // Update emotion
      _currentEmotion = pendingSubtitle.emotion;

      // Record subtitle frame in history
      _speechHistoryManager.recordSubtitleFrame(frameNumber);

      // State detection: subtitle + voice ‚Üí SPEAKING
      _currentDetectedState = ActorState.speaking;

      // Reset speech history when entering speaking state
      _speechHistoryManager.reset();
    }

    // 4. Update speech history for finish detection
    final speechHistoryResult = _speechHistoryManager.update(
      SpeechHistoryInput(
        voiceActivity: smoothedActivity,
        energy: energy,
        frame: frameNumber,
      ),
    );

    // 5. Detect state transitions
    if (speechHistoryResult.shouldFinishSpeaking) {
      // Sustained quiet ‚Üí IDLE
      _currentDetectedState = ActorState.idle;
      _speechHistoryManager.setIdleStateStartFrame(frameNumber);
      _lastSubtitleId = null; // Reset for next turn
    } else if (speechHistoryResult.shouldResumeFromIdle &&
        _currentDetectedState == ActorState.idle) {
      // Strong signal after idle ‚Üí SPEAKING
      _currentDetectedState = ActorState.speaking;
      _speechHistoryManager.reset();
    }

    // 6. Update eyes state (natural blinking)
    final eyesState = _eyesStateManager.update();

    // 7. Update mouth state (audio-driven, state-aware)
    final audioFeatures = AudioFeatureSet(
      energy: energy,
      zcr: zcr,
      spectralCentroid: audioMetrics.spectralCentroid,
      highFreqEnergy: audioMetrics.highFreqEnergy,
    );
    final mouthState = _mouthController.update(audioFeatures);

    // 8. Select character frame (emotion + mouth + eyes)
    final imageUrl = _selectFrame(_currentEmotion, mouthState, eyesState);

    return FrameOutput(
      imageUrl: imageUrl,
      pag: _pagSrc != null ? PAGOutput(src: _pagSrc!, isPlaying: true) : null,
      emoji: _currentEmoji, // üÜï ËøîÂõûÂΩìÂâç emoji
      detectedState: _currentDetectedState,
      currentEmotion: _currentEmotion,
      shouldDisplaySubtitle: shouldDisplaySubtitle,
      displayedSubtitle: displayedSubtitle,
      debug: FrameDebugInfo(
        mouthState: mouthState,
        eyesState: eyesState,
        voiceActivity: smoothedActivity,
        energy: energy,
      ),
    );
  }

  /// Trigger PAG animation for emotion (with random selection)
  void _triggerPAGAnimation(String emotion, String subtitleId) {
    // Check if this is a new subtitle
    if (subtitleId == _lastSubtitleId) {
      return; // Already processed
    }

    // Random selection (reads from global config)
    final selectedAnimation = selectRandomPAG(emotion);

    if (selectedAnimation != null) {
      final filename = selectedAnimation.src.split('/').last;
      debugPrint('[Frame Selector] PAG triggered: $emotion ‚Üí $filename');
      _pagSrc = selectedAnimation.src;
    }

    _lastSubtitleId = subtitleId;
  }

  /// Reset PAG animation (called when PAG animation ends)
  void resetPAG() {
    _pagSrc = null;
  }

  /// Get current detected state (for external coordination)
  ActorState getCurrentState() {
    return _currentDetectedState;
  }

  /// Cleanup resources
  void destroy() {
    _eyesStateManager.destroy();
    _mouthController.reset();
    _pagSrc = null;
  }
}

/// Create a frame selector instance
FrameSelector createFrameSelector({
  required VadConfig vadConfig,
  required PauseDetectionConfig pauseDetectionConfig,
  EyesLifecycleConfig? eyesLifecycleConfig,
  EyesTimingConfig? eyesTimingConfig,
  MouthConfig? mouthConfig,
}) {
  return FrameSelector(
    vadConfig: vadConfig,
    pauseDetectionConfig: pauseDetectionConfig,
    eyesLifecycleConfig: eyesLifecycleConfig,
    eyesTimingConfig: eyesTimingConfig,
    mouthConfig: mouthConfig,
  );
}
