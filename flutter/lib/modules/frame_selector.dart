/// Frame Selector - Unified Animation Frame Selection
///
/// ç»Ÿä¸€çš„å¸§é€‰æ‹©å™¨ï¼Œå°è£…æ‰€æœ‰åŠ¨ç”»çŠ¶æ€æ›´æ–°é€»è¾‘ã€‚
///
/// æ ¸å¿ƒç†å¿µï¼š
/// 1. çº¯å‡½æ•°å¤„ç†æ¯ä¸€å¸§çš„æ•°æ®
/// 2. ä¸ä¾èµ–å¤–éƒ¨çŠ¶æ€æœºï¼Œè€Œæ˜¯æ¨å¯¼å‡ºå½“å‰åº”è¯¥å¤„äºçš„çŠ¶æ€
/// 3. è°ƒç”¨æ–¹è´Ÿè´£åè°ƒçŠ¶æ€æœºäº‹ä»¶è§¦å‘
library;

import 'package:flutter/foundation.dart';

import '../config/eyes_config.dart';
import '../config/mouth_config.dart';
import 'audio_analysis.dart';
import 'emotion_image_sets.dart';
import 'eyes_controller.dart';
import 'mouth_controller.dart';
import 'small_emotion_controller.dart';
import 'speech_history.dart';
import 'types.dart';
import 'vad.dart';

// ============================================================================
// Types
// ============================================================================

/// å½“å‰æ¶ˆæ¯æ•°æ®ï¼ˆåŒ…å«æ‰€æœ‰å…ƒæ•°æ®ï¼‰
class CurrentMessage {
  final String id;
  final String content;
  final String? emotion;
  final String? emoji;
  final int? turnId;
  final int? turnStatus;

  const CurrentMessage({
    required this.id,
    required this.content,
    this.emotion,
    this.emoji,
    this.turnId,
    this.turnStatus,
  });
}

/// è¾“å…¥æ•°æ® - æ¯ä¸€å¸§çš„åŸå§‹æ•°æ®
class FrameInput {
  /// éŸ³é¢‘ç‰¹å¾ï¼ˆæ¥è‡ª AudioAnalyzerï¼‰
  final VoiceMetrics audioMetrics;

  /// å½“å‰æœªæ˜¾ç¤ºçš„æ¶ˆæ¯ï¼ˆåŒ…å«æ‰€æœ‰å…ƒæ•°æ®ï¼‰
  final CurrentMessage? currentMessage;

  /// å½“å‰å¸§ç¼–å·
  final int frameNumber;

  const FrameInput({
    required this.audioMetrics,
    this.currentMessage,
    required this.frameNumber,
  });
}

/// è°ƒè¯•ä¿¡æ¯
class FrameDebugInfo {
  final MouthIntensity mouthState;
  final EyesState eyesState;
  final VoiceActivityState voiceActivity;
  final double energy;

  const FrameDebugInfo({
    required this.mouthState,
    required this.eyesState,
    required this.voiceActivity,
    required this.energy,
  });
}

/// è¾“å‡ºæ•°æ® - æ¸²æŸ“æ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯
class FrameOutput {
  /// è§’è‰²å›¾ç‰‡ URL
  final String imageUrl;

  /// PAG åŠ¨ç”» srcï¼ˆç›´æ¥è¿”å› String?ï¼‰
  final String? pag;

  /// Emoji URLï¼ˆç”¨äº emoji overlay æ˜¾ç¤ºï¼‰
  final String? emoji;

  /// æ£€æµ‹åˆ°çš„ä¼šè¯çŠ¶æ€ï¼ˆæ¨å¯¼ç»“æœï¼‰
  final ActorState detectedState;

  /// æ˜¯å¦æ˜¾ç¤ºæ–°å­—å¹•
  final bool shouldDisplaySubtitle;

  /// åº”è¯¥æ˜¾ç¤ºçš„æ¶ˆæ¯ ID
  final String? messageIdToDisplay;

  /// è°ƒè¯•ä¿¡æ¯
  final FrameDebugInfo? debug;

  const FrameOutput({
    required this.imageUrl,
    this.pag,
    this.emoji,
    required this.detectedState,
    required this.shouldDisplaySubtitle,
    this.messageIdToDisplay,
    this.debug,
  });
}

// ============================================================================
// Frame Selector Class
// ============================================================================

/// Unified frame selector that coordinates all animation subsystems
class FrameSelector {
  final VadConfig vadConfig;
  final PauseDetectionConfig pauseDetectionConfig;

  // Internal Controllers
  late final MouthController _mouthController;
  late final EyesStateManager _eyesStateManager;
  late final VoiceActivityManager _voiceActivityManager;
  late final SpeechHistoryManager _speechHistoryManager;

  // Internal State
  ActorState _currentDetectedState = ActorState.idle;
  // Emotion used for rendering frames (only update when SPEAKING starts)
  String _currentRenderedEmotion = '[peace]';

  // PAG Animation State
  String? _pagSrc;

  // Emoji State
  String? _currentEmoji;
  int? _lastEmotionTriggeredTurnId;

  // Pending emotion trigger (for delayed triggering)
  ({String emotion, String? emoji})? _pendingEmotionTrigger;

  FrameSelector({
    required this.vadConfig,
    required this.pauseDetectionConfig,
    EyesLifecycleConfig? eyesLifecycleConfig,
    EyesTimingConfig? eyesTimingConfig,
    MouthConfig? mouthConfig,
  }) {
    // Initialize controllers with lifecycle state callback
    _mouthController = MouthController(
      getLifecycleState: () => _currentDetectedState,
      vadConfig: vadConfig,
      mouthConfig: mouthConfig,
    );

    _voiceActivityManager = VoiceActivityManager(
      VoiceActivityManagerConfig(
        debounceFrames: vadConfig.debounceFrames,
        smoothingFactor: vadConfig.smoothingFactor,
      ),
      VoiceActivityState.quiet,
    );

    _eyesStateManager = EyesStateManager(
      getLifecycleState: () => _currentDetectedState,
      lifecycleConfig: eyesLifecycleConfig,
      timingConfig: eyesTimingConfig,
    );

    _speechHistoryManager = SpeechHistoryManager(
      vadConfig: vadConfig,
      pauseDetectionConfig: pauseDetectionConfig,
    );
  }

  /// Select animation frame based on emotion, mouth state, and eyes state (pure method)
  String _selectFrame(
    String emotion,
    MouthIntensity mouthState,
    EyesState eyesState,
  ) {
    // 1. Get emotion image set with fallback to [peace]
    var imageSet = getEmotionImageSet(emotion);
    if (imageSet == null) {
      debugPrint("[ANIMATION] Emotion '$emotion' not found, falling back to [peace]");
      imageSet = getEmotionImageSet('[peace]');
      if (imageSet == null) {
        throw Exception(
          "[ANIMATION] Critical: Default emotion '[peace]' not found - backend data integrity error",
        );
      }
    }

    // 2. Select image based on eyes and mouth state
    final eyesClosed = eyesState == EyesState.closed;
    final mouthClosed = mouthState == MouthIntensity.closed;

    if (eyesClosed && !mouthClosed) {
      return imageSet.eyesClosedMouthOpen;
    } else if (eyesClosed && mouthClosed) {
      return imageSet.eyesClosedMouthClosed;
    } else if (!eyesClosed && !mouthClosed) {
      return imageSet.eyesOpenMouthOpen;
    } else {
      return imageSet.eyesOpenMouthClosed;
    }
  }

  /// Process a single frame and return rendering output
  FrameOutput processFrame(FrameInput input) {
    final audioMetrics = input.audioMetrics;
    final currentMessage = input.currentMessage;
    final frameNumber = input.frameNumber;

    final energy = audioMetrics.energy;
    final zcr = audioMetrics.zcr;

    // Incoming emotion from latest message (do NOT render immediately; wait until SPEAKING starts)
    final incomingEmotion = currentMessage?.emotion ?? '[peace]';

    // 1. Voice activity detection
    final newVoiceActivity = classifyVoiceActivity(energy, zcr, vadConfig);
    final smoothedActivity = _voiceActivityManager.smooth(newVoiceActivity);

    // 2. å¤„ç† turnStatus === 1ï¼ˆæƒ…ç»ªè§¦å‘ï¼‰
    final turnStatus = currentMessage?.turnStatus ?? 0;
    final turnId = currentMessage?.turnId;

    // Use turnId-based dedupe (instead of 0â†’1 edge) to avoid missing triggers
    final shouldTriggerForThisTurn =
        turnStatus == 1 && turnId != null && turnId != _lastEmotionTriggeredTurnId;

    if (shouldTriggerForThisTurn) {
      final emotion = incomingEmotion;
      final emoji = currentMessage?.emoji;

      if (_currentDetectedState == ActorState.speaking) {
        // å·²åœ¨ SPEAKING çŠ¶æ€ â†’ ç«‹å³è§¦å‘
        _triggerEmotionAnimation(emotion, emoji);
        debugPrint('âœ… [EMOTION] Triggered immediately (already SPEAKING)');
      } else {
        // è¿˜åœ¨ IDLE çŠ¶æ€ â†’ æš‚å­˜ï¼Œç­‰ SPEAKING å¼€å§‹æ—¶è§¦å‘
        _pendingEmotionTrigger = (emotion: emotion, emoji: emoji);
        debugPrint('â³ [EMOTION] Queued for SPEAKING start');
      }

      _lastEmotionTriggeredTurnId = turnId;
    }

    // 3. å­—å¹•æ˜¾ç¤ºåˆ¤æ–­ + çŠ¶æ€è½¬æ¢ + è§¦å‘ pending
    bool shouldDisplaySubtitle = false;
    String? messageIdToDisplay;

    if (currentMessage != null && newVoiceActivity == VoiceActivityState.active) {
      shouldDisplaySubtitle = true;
      messageIdToDisplay = currentMessage.id;
      _speechHistoryManager.recordSubtitleFrame(frameNumber);

      // çŠ¶æ€è½¬æ¢ï¼šIDLE â†’ SPEAKING
      final wasIdle = _currentDetectedState == ActorState.idle;
      _currentDetectedState = ActorState.speaking;
      _speechHistoryManager.reset();

      // Only update rendered emotion when we actually start SPEAKING / display subtitle
      _currentRenderedEmotion = incomingEmotion;

      // æ£€æµ‹ SPEAKING å¼€å§‹ï¼Œè§¦å‘ pending çš„æƒ…ç»ªåŠ¨ç”»
      if (wasIdle && _pendingEmotionTrigger != null) {
        _triggerEmotionAnimation(
          _pendingEmotionTrigger!.emotion,
          _pendingEmotionTrigger!.emoji,
        );
        _pendingEmotionTrigger = null;
        debugPrint('âœ… [EMOTION] Triggered at SPEAKING start');
      }
    }

    // 4. Speech history update (finish detection)
    final speechHistoryResult = _speechHistoryManager.update(
      SpeechHistoryInput(
        voiceActivity: smoothedActivity,
        energy: energy,
        frame: frameNumber,
      ),
    );

    // ä»…åœ¨"ç¡®å®è¿›å…¥è¿‡ SPEAKING"æ—¶æ‰å…è®¸ finish ç”Ÿæ•ˆ
    if (_currentDetectedState == ActorState.speaking &&
        speechHistoryResult.shouldFinishSpeaking) {
      debugPrint('[SUBTITLE] shouldFinishSpeaking');
      _currentDetectedState = ActorState.idle;
      _pendingEmotionTrigger = null; // æ¸…ç©º pending
      _currentEmoji = null;
    }

    // 5. Eyes & mouth state
    final eyesState = _eyesStateManager.update();
    final mouthState = _mouthController.update(
      AudioFeatureSet(
        energy: energy,
        zcr: zcr,
        spectralCentroid: audioMetrics.spectralCentroid,
        highFreqEnergy: audioMetrics.highFreqEnergy,
      ),
    );

    // 6. Select frame (emotion only updates when SPEAKING starts)
    final imageUrl = _selectFrame(_currentRenderedEmotion, mouthState, eyesState);

    return FrameOutput(
      imageUrl: imageUrl,
      pag: _pagSrc,
      emoji: _currentEmoji,
      detectedState: _currentDetectedState,
      shouldDisplaySubtitle: shouldDisplaySubtitle,
      messageIdToDisplay: messageIdToDisplay,
      debug: FrameDebugInfo(
        mouthState: mouthState,
        eyesState: eyesState,
        voiceActivity: smoothedActivity,
        energy: energy,
      ),
    );
  }

  /// è§¦å‘æƒ…ç»ªåŠ¨ç”»ï¼ˆEmoji ä¼˜å…ˆï¼Œå¦åˆ™ PAGï¼‰
  void _triggerEmotionAnimation(String emotion, String? emoji) {
    if (emoji != null) {
      // æœ‰ emojiï¼šè®¾ç½® emojiï¼Œè·³è¿‡ PAG
      _currentEmoji = emoji;
      debugPrint('ğŸ˜Š [EMOJI] Set emoji: $emoji');
      debugPrint('ğŸš« [PAG] Skipped due to emoji priority');
    } else {
      // æ—  emojiï¼šæ¸…ç©º emojiï¼Œè§¦å‘ PAG æŠ½å¥–
      _currentEmoji = null;
      final selectedAnimation = selectRandomPAG(emotion);
      if (selectedAnimation != null) {
        _pagSrc = selectedAnimation.src;
        final filename = selectedAnimation.src.split('/').last;
        debugPrint('ğŸ¨ [PAG] Triggered: $emotion â†’ $filename');
      }
    }
  }

  /// Reset PAG animation (called when PAG animation ends)
  void resetPAG() {
    _pagSrc = null;
  }

  /// Reset Emoji (called when Emoji overlay animation ends)
  void resetEmoji() {
    _currentEmoji = null;
  }

  /// Get current detected state (for external coordination)
  ActorState getCurrentState() {
    return _currentDetectedState;
  }

  /// Cleanup resources
  void destroy() {
    _eyesStateManager.destroy();
    _mouthController.reset();
    _pagSrc = null;
  }
}

/// Create a frame selector instance
FrameSelector createFrameSelector({
  required VadConfig vadConfig,
  required PauseDetectionConfig pauseDetectionConfig,
  EyesLifecycleConfig? eyesLifecycleConfig,
  EyesTimingConfig? eyesTimingConfig,
  MouthConfig? mouthConfig,
}) {
  return FrameSelector(
    vadConfig: vadConfig,
    pauseDetectionConfig: pauseDetectionConfig,
    eyesLifecycleConfig: eyesLifecycleConfig,
    eyesTimingConfig: eyesTimingConfig,
    mouthConfig: mouthConfig,
  );
}
